%!TEX root = disclosure_game.tex
\section{Model}
\label{sec:model}
%Start this off with a brief description of our fictional scenario.
% Whole section ~ 1.5K

\subsection{Disclosure Game}
\label{sub:the_game}
%Outline how the scenario translates into a game.
%Brief mention of the game theoretic solution

In order to translate the scenario sketched above into a more abstract, tractable form, we cast it as a signalling game, and assume that women's disclosures (or not), are signals. We also make the simplifying assumption that a woman may have one of only three drinking patterns - light, moderate, or heavy. Correspondingly, they are limited in what signals they may send to claiming to be one of these three types.

Midwives are treated in a similar fashion, where their type corresponds to how negatively they regard a drinking pattern - non-judgemental, moderately judgemental, and harshly judgemental. The expression of this judgement is not a matter of choice on their part, and is assumed not to impact on their choice of response, which is either to refer the woman for specialist treatment, or to do nothing.

At the end of a game, each player receives a payoff dependent on the actions and types of both players, which has a partially common interest component. Women receive a payoff based on the health of their eventual baby, with a social cost dependent on the signal they sent and the midwife's reaction to it. Midwives receive the same health payoff as the women, but pay a cost for referring to a specialist, mirroring the organisational cost of non-routine care.



Agents treat this two player game as taking place against nature, along the lines of adversial risk analysis \citep{THINGY}. 

%Might belong in a more methody bit? Read some stuff to check..
Women are drawn in order from a queue, and play against a midwife chosen at random. They play for a maximum of 12 rounds (following the routine number of ante-natal appointments in the UK \citep{NICE2010a}) or until they are referred. At which point a new player is drawn from the same distribution that produced the original players to replace them. If they are not referred, they rejoin the back of the queue after their appointment. In either case, they are informed of their payoff after each round and update their beliefs accordingly.

Midwives play for \(k\) rounds (\(k=1000\) in all experiments), and conduct appointments in parallel. Unlike women, midwives are only informed of their payoff if they choose to make a referral. Both groups of agents have perfect recall, and midwives are assumed to retrospectively update their observations if they make a referral after a number of appointments.


\subsection{Agent Models}
\label{sub:the_agents}
%Outline basic structure, then specifics on each one.
%This should probably go lexic -> bayes payoff -> bayes -> CPT in order
% of complexity.
%suggest there are added shells of complexity
%		mention the info sharing because homophilly
%			related - the enhancement to the bayes/cpt agents is that they personalise, as much as anything.
%		properly explain dirichlet priors
% How about comparing the decision problem representation for the agent types?

While in principle a wide variety of agent models, given that decision rules operate on essentially the same information, and produce the same outputs, we limit ourselves here to four. The simplest is a lexicographic rule (1), motivated as in the spirit of a \ac{FFH} \citep{Gigerenzer2004} which uses only information about payoffs given actions; a Bayesian risk minimisation rule using the same information (2); a second Bayesian risk rule (3) which uses information about the underlying lottery; and a two-stage \ac{CPT} \cite{Hau2008} agent (4) which is identical with 3, but uses the \ac{CPT} decision rule from \cite{Tversky1992}.

As noted in section \ref{sub:the_game}, agents have perfect recall, and recognise individual opponents if they encounter them subsequently. While agents recall perfectly and make use of the new information for retrospective updates, their decisions are made \'as-if\' they were facing a new opponent.

\subsubsection{Payoff Reasoning}

Both payoff reasoners assume a simplified decision problem, as in figure \ref{fig:payoff_problem}, where an action is a choice between lotteries. The lexicographic heuristic (algorithm \ref{alg:lexicographic}) maintains a count of the number of times that each action was followed by a payoff, and chooses the action which most commonly has the best payoff, i.e. one reason decision making. The Bayesian risk minimisation agent uses the same subset of information, but updates beliefs on the link between actions and payoffs using Bayes rule, and attempts to choose the action which minimises risk.

\subsubsection{Bayesian Risk Minimisation}

Aside from the heuristic agent, the remaining three agent types use Bayesian updating to attempt to improve
the accuracy of their beliefs. Given the discrete nature of agent
types, and actions, coupled with a desire for tractability of the
simulation, the Dirichlet distribution is employed. The probability
density function takes the form -

\[
D(\Theta|\alpha)=\frac{\Gamma(\sum_{i=1}^{k}\alpha_{i})}{\prod_{i=1}^{k}\Gamma(\alpha_{i})}\overset{k}{\underset{i=1}{\prod}\Theta_{i}^{\alpha_{i-1}}}
\]


Where \(\alpha=\{\alpha_{1}\ldots\alpha_{k}\}\), \(k\)is the number
of discrete types, \(\Theta=\{x_{1},\,\ldots,x_{k-\text{1}}\}\) all
more than zero and summing to less than 1, and \(\alpha_{i}\) is the
psuedo-count of prior observations for type \(i\). 

The distribution is particularly convenient, in that to infer the
probability of a new observation being of some particular type becomes
simply -

\begin{equation}
P(x=j|D,\alpha)=\frac{\alpha_{j}+n_{j}}{\sum_{j}(\alpha_{j}+n_{j})}\label{eq:posterior}
\end{equation}


Where \(n_{j}\) is simply the count of occurrences of type \(j\), so
that the belief that a new observation is of type \(j\) the number
of times that type has been observed (including the pseudo-count),
over the total number of observations thus far. This makes computation
of beliefs fast, and simple, since all that must be maintained is
a count of observations with no particular concern as to their order. 

\subsubsection{Descriptive Decision Theory}

\subsection{Information Sharing}
\label{sub:info_sharing}

It would seem unreasonable to suppose that neither party recounts their experiences to their peers, and to explore the impact of this we also modify the game to introduce a simple form of information sharing within agent groups. This takes the form of having each agent share their memories with their colleagues with some probability \(p\). Individuals then incorporate shared information into their beliefs using weighted updates, such that a shared observation of a low type signal contributes to their beliefs by \(q\), and \(0\leq q\leq 1\). Women share only when they have finished play, and provide their complete history of games, because they have accurate information about the outcomes. By the same rationale, midwives share only their history with the most recent woman they referred. Sharing occurs simultaneously for all players at the end of each round, and all memories are either shared immediately or discarded.\footnote{Memories of games remain, but the assumption is that only current news is relevant.}

Because of their differing problem representations, the simple payoff reasoners and their more complex counterparts incorporate this outside information differently. The simple payoff based rule relys on a belief structure relating actions directly to rewards. Because payoffs differ by the agent's private type, the information shared may not correspond to the experience of the listening agent in the same scenario. As a result, payoff reasoners have a belief bias towards the most common player type, and can believe in outcomes that are, for them, impossible.

By contrast, representing the problem in terms of the probabilities of the individual lotteries yields a structure that generalises the new information.

\subsection{Model Implementation}
\label{sub:the_code}
% Use this bit to talk about the practical version of the game and where info is revealed?
The model itself is implemented as a Python module, and is freely available under the Mozilla Public License from \url{https://github.com/greenape/disclosure-game}, together with full parameter sets, raw data, and all other code used in producing this paper.