\section{Agent Examples}
\label{sec:agent_eg}

This section provides a worked example for the learning and decision process of each agent model, focusing on the behaviour of the signalling agent.

\subsection{Lexicographic Heuristic}
\label{sub:lexico_eg}

As an example, take a light drinker who has played three rounds with a succession of particularly judgemental midwives, signalling honestly in two and claiming to be a moderate drinker in one. The most common outcome of the honest signal was a payoff of 10, which is clearly preferable to the 9 gained by claiming to be moderate. On that basis, they choose to signal honestly.

\subsection{Bayesian Payoff}
\label{sub:payoff_eg}

If we take our light drinker from the lexicographic case, and assume that they began with an uninformative prior. The 6 possible signal-payoffs pairings are then \([(l,10),(m,10),(h,10),(m,9),(h,9),(h,8)]\), with \(\alpha_{i}=1\) for all \(i\). After playing the three rounds, \(n_{l,10}=2\), and \(n_{m,9}=1\).

The agent then evaluates \(R_{w}\) for each signal, e.g. for the light signal:

\begin{equation*}
\begin{aligned}
X &= \{10\}\\
R_{w}(l) &= \sum_{x \in X} -xp(x | l) = -10p(10 | l)\\
R_{w}(l) &= -10(\frac{\alpha_{l,10}+n_{l,10}}{\sum_{j}(\alpha_{j}+n_{j})}) = -10(\frac{1+2}{1+2})\\
R_{w}(l) &= -10(\frac{3}{3}) = -10
\end{aligned}
\end{equation*}

and by the same method, \(R_{w}(m)=-9\frac{1}{3}\), and \(R_{w}(h)=-9\), concluding that signalling honestly is the best move.

\subsection{Bayesian Risk Minimisation}
\label{sub:bayes_eg}

Returning to our example agent, under this model the type of the midwife becomes salient, hence \(n_{h}=3\), and \(n_{l,n}=2\), \(n_{m,n}=1\). Their prior beliefs remain uninformative, i.e. \(\alpha_{j} = 1, j \in \{l,m,h\}\), \(\alpha_{i,j}=1,i \in \{r,n\}, j \in \{l,m,h\}\). As before, the agent evaluates \(R_{w}\) for the three signals, and the process for the light signal is given below -

\begin{equation*}
\begin{aligned}
R_{w}(l, l) &= \sum_{i\in A_{m}}\sum_{j\in \Theta} -u_{w}(l, i, l, j)p(j)p(i | l)\\
R_{w}(l, l) &= -u_{w}(l, r, l, l)p(l)p(r | l) - u_{w}(l, n, l, l)p(l)p(n | l) - u_{w}(l, r, l, m)p(m)p(r | l) - u_{w}(l, n, l, m)p(m)p(n | l) \\
&\phantom{{}=1}- u_{w}(l, r, l, h)p(h)p(r | l) - u_{w}(l, n, l, h)p(h)p(n | l)\\
u_{w}(l, i, l, j) &= 10\\
R_{w}(l, l) &= -10p(l)p(r | l) - 10p(l)p(n | l) - 10p(m)p(r | l) - 10p(m)p(n | l) - 10p(h)p(r | l) - 10p(h)p(n | l)\\
p(l) &= \frac{1 + 0}{1 + 1 + 1 + 3} = \frac{1}{6}\\
p(m) &= \frac{1 + 0}{1 + 1 + 1 + 3} = \frac{1}{6}\\
p(h) &= \frac{1 + 3}{1 + 1 + 1 + 3} = \frac{2}{3}\\
p(r | l) &= \frac{1 + 0}{1 + 1 + 2} = \frac{1}{4}\\
p(n | l) &= \frac{1 + 2}{1 + 1 + 2} = \frac{3}{4}\\
R_{w}(l, l) &= -10\cdot \frac{1}{6} \cdot \frac{1}{4} - 10\cdot \frac{1}{6} \cdot \frac{3}{4} - 10\cdot \frac{1}{6} \cdot \frac{1}{4} - 10\cdot \frac{1}{6} \cdot \frac{3}{4} - 10\cdot \frac{2}{3} \cdot \frac{1}{4} - 10\cdot \frac{2}{3} \cdot \frac{3}{4}\\
&= -10
\end{aligned}
\end{equation*}

and similarly for moderate (\(R_{w}(m,l)=-9\frac{1}{3}\)), and heavy (\(R_{w}(h,l)=-8\frac{1}{2}\)) signals, once again concluding that honesty is the better option.

\subsection{Descriptive Decision Theory}
\label{sub:cpt_eg}

Once again, we return to the light drinker example.  The inferential aspects are identical with the more complex Bayesian risk minimisation algorithm, hence \(p(j)p(i | l)\), and \(u_{w}(l, i, l, j)\) remain the same, but the agent additionally calculates $v(u_{w}(l, i, l, j))w^{+}(p(j))w^{+}(p(i | l))$. For the \ac{CPT} parameters, the values are those originally given by \citet{Tversky1992} and used in the actual simulations which are given fully in table \ref{tab:cpt_params}.

\begin{table}
\begin{tabular}{|l | l | c|}
\hline
Name & Description & Value \\ \hline
\(\gamma\) & Probability weighting for gains  & 0.61 \\ \hline
\(\delta\) & Probability weighting for losses &  0.69\\ \hline
\(\alpha\) & Power for gains  & 0.88 \\ \hline
\(\beta\) & Power for losses & 0.88 \\ \hline
\(\lambda\) & Loss aversion &  2.25 \\ \hline
\end{tabular}
\caption[Table caption text]{\ac{CPT} parameters. \label{tab:cpt_params}}
\end{table}

\begin{equation*}
\begin{aligned}
\alpha &= 0.88\\
\gamma &= 0.61\\
p(l) &= \frac{1}{6}\\
p(m) &= \frac{1}{6}\\
p(h) &= \frac{2}{3}\\
p(r | l) &= \frac{1}{4}\\
p(n | l) &= \frac{3}{4}\\
u_{w}(l, i, l, j) &= 10\\
f&=(10;\frac{1}{24},10;\frac{1}{8},10;\frac{1}{24},10;\frac{1}{8},10;\frac{1}{6},10;\frac{1}{2})\\
f^{+}&=f,f^{-}=()\\
n &= 5\\
v(u_{w}) &= f(u_{w}) = u_{w}^\alpha\\
v(u_{w}) &= 10^{0.88} = 7.59\\
\pi_{0}^{+}&=w^{+}(\frac{1}{24} + \frac{1}{8} + \frac{1}{24} + \frac{1}{8} + \frac{1}{6} + \frac{1}{2}) - w^{+}(\frac{1}{8} + \frac{1}{24} + \frac{1}{8} + \frac{1}{6} + \frac{1}{2})=w^{+}(1) - w^{+}(\frac{23}{24})\\
&=0.19\\
\pi_{1}^{+}&=w^{+}(\frac{1}{8} + \frac{1}{24} + \frac{1}{8} + \frac{1}{6} + \frac{1}{2}) - w^{+}(\frac{1}{24} + \frac{1}{8} + \frac{1}{6} + \frac{1}{2})=w^{+}(\frac{23}{24}) - w^{+}(\frac{5}{6})\\
&= 0.17\\
\pi_{2}^{+}&=w^{+}(\frac{1}{24} + \frac{1}{8} + \frac{1}{6} + \frac{1}{2}) - w^{+}(\frac{1}{8} + \frac{1}{6} + \frac{1}{2})=w^{+}(\frac{5}{6}) - w^{+}(\frac{19}{24})\\
&= 0.04\\
\pi_{3}^{+}&=w^{+}(\frac{1}{8} + \frac{1}{6} + \frac{1}{2}) - w^{+}(\frac{1}{6} + \frac{1}{2})=w^{+}(\frac{19}{24}) - w^{+}(\frac{2}{3})\\
&= 0.09\\
\pi_{4}^{+}&=w^{+}(\frac{1}{6} + \frac{1}{2}) - w^{+}(\frac{1}{2})=w^{+}(\frac{2}{3}) - w^{+}(\frac{1}{2})\\
&= 0.09\\
\pi_{5}^{+}&=w^{+}(\frac{1}{2})\\
&= 0.42\\
V(f) &= V(f^{+}) + V(f^{-}) = V(f^{+}) + 0\\
V(f^{+}) &= \sum_{i}^{n}\pi_{i}^{+}(f^{+})v_{i}^{+}(f^{+}) = 7.59
\end{aligned}
\end{equation*}


And as before, following the same process for moderate, and heavy signals which yields respectively 7.14, and 6.22, the agent chooses the higher valued action and sends an honest signal.